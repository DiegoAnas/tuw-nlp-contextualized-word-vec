{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "import NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://tanmay17061.medium.com/load-pre-trained-glove-embeddings-in-torch-nn-embedding-layer-in-under-2-minutes-f5af8f57416a\n",
    "# \n",
    "embeddings_dict = {}\n",
    "with open(f\"glove.6B.{dim}d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "global_vectors = GloVe(name='6B', dim=embed_dim)\n",
    "glove_weights = torch.load(f\".vector_cache/glove.6B.{embed_dim}d.txt.pt\")\n",
    "emb_layer = nn.Embedding.from_pretrained(glove_weights[2], freeze=True, padding_idx=NMT.Constants.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dim = 100\n",
    "lstm_layers = 2\n",
    "dropout = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "#training params\n",
    "epochs = 20\n",
    "learning_rate = 1\n",
    "optimizer = \"SGD\"\n",
    "#Next options are outdated in favour of optimizer schedulers?\n",
    "#max_grad_norm\n",
    "#learning_rate_decay\n",
    "#start_decay_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/dbmdz/bert-base-german-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer_de = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def encode_trans(examples):\n",
    "  examples = examples[\"translation\"]\n",
    "  ens = []\n",
    "  des = []\n",
    "  for ex in examples:\n",
    "    # possible filter short sentences so no padding is needed\n",
    "      ens.append(ex['en'])\n",
    "      des.append(ex['de'])\n",
    "  inputs = tokenizer_en(ens, padding='longest', truncation=True, max_length=40)\n",
    "  targets = tokenizer_de(des, padding='longest', truncation=True, max_length=40)\n",
    "  return {'input': inputs[\"input_ids\"], \"target\": targets[\"input_ids\"]}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_custom(batch):\n",
    "  inputs = batch[0][\"input\"]\n",
    "  targets = batch[0][\"target\"]\n",
    "  return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stream = load_dataset(\"wmt16\", \"de-en\", streaming=True, split=\"train\", trust_remote_code=True)\n",
    "dataset_batched = dataset_stream.batch(batch_size=32)\n",
    "dataset_m = dataset_batched.map(encode_trans,remove_columns=\"translation\")\n",
    "train_dataloader = DataLoader(dataset_m, collate_fn=collate_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NMT import Models\n",
    "\n",
    "encoder = Models.Encoder(num_layers=lstm_layers, bidirectional=True, dropout=dropout, rnn_size=dim)\n",
    "decoder = Models.Decoder(num_layers=lstm_layers, bidirectional=False, dropout=dropout, rnn_size=dim)\n",
    "model = Models.NMTModel(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
