{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import NMT\n",
    "from NMT.utils import get_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://tanmay17061.medium.com/load-pre-trained-glove-embeddings-in-torch-nn-embedding-layer-in-under-2-minutes-f5af8f57416a\n",
    "# \n",
    "embeddings_dict = {}\n",
    "with open(f\"glove.6B.{dim}d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "global_vectors = GloVe(name='6B', dim=embed_dim)\n",
    "glove_weights = torch.load(f\".vector_cache/glove.6B.{embed_dim}d.txt.pt\")\n",
    "emb_layer = nn.Embedding.from_pretrained(glove_weights[2], freeze=True, padding_idx=NMT.Constants.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dim = 100\n",
    "lstm_layers = 2\n",
    "dropout = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "#training params\n",
    "epochs = 20\n",
    "learning_rate = 1\n",
    "optimizer = \"SGD\"\n",
    "#Next options are outdated in favour of optimizer schedulers?\n",
    "#max_grad_norm\n",
    "#learning_rate_decay\n",
    "#start_decay_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "# https://huggingface.co/dbmdz/bert-base-german-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer_de = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NMT import Models\n",
    "\n",
    "encoder = Models.Encoder(num_layers=lstm_layers, bidirectional=True, dropout=dropout, rnn_size=dim)\n",
    "decoder = Models.Decoder(num_layers=lstm_layers, bidirectional=False, dropout=dropout, rnn_size=dim)\n",
    "model = Models.NMTModel(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_epoch \u001b[38;5;129;01min\u001b[39;00m num_epochs:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_en, target_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_de, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     10\u001b[0m     test_dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_en, target_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_de, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from NMT.utils import run_evaluation, train\n",
    "\n",
    "\n",
    "hidden_size = 50\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "for num_epoch in num_epochs:\n",
    "    #input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "    train_dataloader = get_dataloader(split=\"train\", input_tokenizer=tokenizer_en, target_tokenizer=tokenizer_de, batch_size=batch_size)\n",
    "    test_dataloader = get_dataloader(split=\"test\", input_tokenizer=tokenizer_en, target_tokenizer=tokenizer_de, batch_size=batch_size)\n",
    "\n",
    "    #TODO encoder decoder options\n",
    "    encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "    decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "    train(train_dataloader, encoder, decoder, num_epoch, print_every=5, plot_every=5)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "run_evaluation(encoder, decoder, test_pairs, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
