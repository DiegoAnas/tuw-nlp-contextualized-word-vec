{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import NMT\n",
    "from NMT.utils import get_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "num_epochs = 1\n",
    "#learning_rate = 1\n",
    "#optimizer = \"SGD\"\n",
    "#Next options are outdated in favour of optimizer schedulers?\n",
    "#max_grad_norm\n",
    "#learning_rate_decay\n",
    "#start_decay_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change tokenizer tokens (adapt constatns to IDS?)\n",
    "# bos_token = constants.BOS_WORD, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token\n",
    "tokenizer_en = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# https://huggingface.co/dbmdz/bert-base-german-cased\n",
    "tokenizer_de = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for num_epoch in num_epochs:\n",
    "    #input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "train_dataloader = get_dataloader(split=\"train\", input_tokenizer=tokenizer_en, target_tokenizer=tokenizer_de, batch_size=batch_size)\n",
    "test_dataloader = get_dataloader(split=\"test\", input_tokenizer=tokenizer_en, target_tokenizer=tokenizer_de, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NMT import models\n",
    "#num_layers,\n",
    "# bidirectional,\n",
    "# dropout,\n",
    "#'-rnn_size', default=600, \n",
    "#'-word_vec_dim', default=400, # ONMT opt.word_vec_size\n",
    "# dict_size # opt.data -> dict -> dict.size() \n",
    "#Parameters\n",
    "rnn_size = 100\n",
    "word_vec_dim = 50\n",
    "lstm_layers = 2\n",
    "dropout = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "encoder = models.Encoder(num_layers=lstm_layers, bidirectional=True, dropout=dropout, rnn_size=rnn_size, word_vec_dim=word_vec_dim, dict_size=len(tokenizer_en), padding = tokenizer_en.convert_tokens_to_ids(tokenizer_en.pad_token))\n",
    "decoder = models.Decoder(num_layers=lstm_layers, bidirectional=False, dropout=dropout, rnn_size=rnn_size, word_vec_dim=word_vec_dim, dict_size=len(tokenizer_de), padding = tokenizer_en.convert_tokens_to_ids(tokenizer_en.pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmtModel = models.NMTModel(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NMT.utils import train\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 1, print_every=5, plot_every=5)#learning rate # TODO add other optimizers\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
