{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://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@github.com/DiegoAnas/tuw-nlp-contextualized-word-vec.git\n",
    "%cd tuw-nlp-contextualized-word-vec\n",
    "!git checkout devD\n",
    "!git pull\n",
    "#copy the library to the same directory as this notebook\n",
    "!cp -r NMT .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import NMT\n",
    "from NMT.utils import get_dataloader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "sentence_length = 40\n",
    "#learning_rate = 1\n",
    "#optimizer = \"SGD\"\n",
    "#Next options are outdated in favour of optimizer schedulers?\n",
    "#max_grad_norm\n",
    "#learning_rate_decay\n",
    "#start_decay_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO change tokenizer tokens (adapt constatns to IDS?)\n",
    "# bos_token = constants.BOS_WORD, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token\n",
    "tokenizer_en = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# https://huggingface.co/dbmdz/bert-base-german-cased\n",
    "tokenizer_de = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for num_epoch in num_epochs:\n",
    "    #input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "#TODO device\n",
    "train_dataloader = get_dataloader(split=\"train\", input_tokenizer=tokenizer_en,\n",
    "                                  target_tokenizer=tokenizer_de,\n",
    "                                  batch_size=batch_size,\n",
    "                                  sentence_length=sentence_length,\n",
    "                                  streaming=False,\n",
    "                                  shard=100)\n",
    "# 1/100 of dataset = 50k, 20k more than small Dataset\n",
    "# Medium is 200k\n",
    "# Large is 7M ~ full dataset is 4,5M\n",
    "\"\"\"test_dataloader = get_dataloader(split=\"test\", input_tokenizer=tokenizer_en,\n",
    "                                 target_tokenizer=tokenizer_de,\n",
    "                                 batch_size=batch_size,\n",
    "                                 sentence_length=sentence_length)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NMT import models\n",
    "#num_layers,\n",
    "# bidirectional,\n",
    "# dropout,\n",
    "#'-rnn_size', default=600, \n",
    "#'-word_vec_dim', default=400, # ONMT opt.word_vec_size\n",
    "# dict_size # opt.data -> dict -> dict.size() \n",
    "#Parameters\n",
    "rnn_size = 100\n",
    "word_vec_dim = 50\n",
    "lstm_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "encoder = models.Encoder(num_layers=lstm_layers,\n",
    "                         bidirectional=True,\n",
    "                         dropout = dropout, \n",
    "                         rnn_size = rnn_size,\n",
    "                         word_vec_dim = word_vec_dim,\n",
    "                         dict_size = len(tokenizer_en),\n",
    "                         padding = tokenizer_en.convert_tokens_to_ids(tokenizer_en.pad_token))\n",
    "decoder = models.Decoder(num_layers=lstm_layers,\n",
    "                         bidirectional=False,\n",
    "                         dropout=dropout,\n",
    "                         rnn_size=rnn_size,\n",
    "                         word_vec_dim=word_vec_dim,\n",
    "                         dict_size=len(tokenizer_de),\n",
    "                         padding = tokenizer_en.convert_tokens_to_ids(tokenizer_en.pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmtModel = models.NMTModel(encoder,decoder,rnn_size,len(tokenizer_de)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NMT.utils import train\n",
    "\n",
    "train(train_dataloader, nmtModel, n_epochs=1, learning_rate=0.001, print_every=5, plot_every=5, device=device)#learning rate # TODO add other optimizers\n",
    "\n",
    "nmtModel.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
